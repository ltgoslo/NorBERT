# NorBERT
This repository contains in-house code used in training NorBERT: a large-scale Transformer-based language model for Norwegian. The model was trained by the [Language Technology Group](https://www.mn.uio.no/ifi/english/research/groups/ltg/) at the University of Oslo. The computations were performed on resources provided by UNINETT Sigma2 - the National Infrastructure for High Performance Computing and Data Storage in Norway.

For general training, [BERT For TensorFlow from NVIDIA](https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT) was used. We made minor changes to their code, to be published later. 

- [Read about NorBERT](http://wiki.nlpl.eu/index.php?title=Vectors/norlm/norbert)
- [Download NorBERT](http://vectors.nlpl.eu/repository/)
